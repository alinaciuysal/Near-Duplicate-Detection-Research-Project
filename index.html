<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Bilkent University CS490 Research Project : ">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Bilkent University CS490 Research Project</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/alinaciuysal/ndd_project">View on GitHub</a>

          <h1 id="project_title">Bilkent University CS490 Research Project</h1>
          <h2 id="project_tagline"></h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/alinaciuysal/ndd_project/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/alinaciuysal/ndd_project/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h1>
<a id="semantic-duplicate-detection-of-tweets" class="anchor" href="#semantic-duplicate-detection-of-tweets" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Semantic-duplicate Detection of Tweets</h1>

<p>Duplicate document detection topic has been studied for years in the information retrieval field of Computer Science. There exist "state-of-art" algorithms for detecting near-duplicate documents efficiently.</p>

<p>In this project, the duplicate tweets posted on Twitter in a specific time-line will be de-duplicated by using a unique algorithm.</p>

<p>Student Name: Ali Naci Uysal</p>

<p>Project Supervisor: Prof. Fazlı Can</p>

<h2>
<a id="related-papers" class="anchor" href="#papers" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Related Papers</h2>

<p><a href="http://livingknowledge.europarchive.org/images/publications/semAwareNDD.pdf">Efficient Semantic-Aware Detection of Near Duplicate Resources</a></p>
<p>The Semantic Web provides a common framework that allows data to be shared and reused across application, enterprise, and community boundaries. 
Meta-data of resources (news articles, blog postings etc.) are stored in RDF (Resource Description Framework) format. Calais Web Service can extract following information from resources:
Entities, relationships, facts, events, and topics. Calais Web Service then processes the information extracted from the text and returns semantic metadata in RDF format.
The RDF structure of news articles are stored in a graph structure in this article. Locality Sensitive Hashing (LSH) and Jaccard Coefficient is applied to find the similarities of these graphs in order not to do pairwise comparisons. 
The indexing structure also makes their algorithm more efficient.
</p>

<p><a href="http://www2013.wwwconference.org/proceedings/p1273.pdf">Groundhog Day: Near-Duplicate Detection on Twitter</a></p>
<p> In this article, Tweet pairs are examined by using following features:
<p> Syntactic features: Manhattan Distance, overlap in terms of tweets, in hash tags, URLs, tweet length difference </p>
<p> Semantic features: Overlap in entities, entity types, topics (using DBpedia Spotlight and OpenCalais), WordNet concepts and WordNet similarity </p>  
<p> Enriched Semantic Features are constructed as in the same way as semantic features, but it also includes contents of web sites that are linked from the tweets. </p>
<p> Contextual Features: Temporal difference between tweet pairs, user similarity (the differences in #followers and #followees) </p>
WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. The authors identified nouns in the pairs of tweets and calculated their overlap in these nouns by using Java Wordnet Interface to find the root concepts of the nouns in the tweet pairs.
It is stated that, “overlap of WordNet concepts” is a good criterion to detect duplicate tweets compared to other features. </p>
</p>

<p><a href="http://www.cs.bilkent.edu.tr/%7Ecanf/publications/CoDet%20CIKM%202011.pdf">CoDet: Sentence-based Containment Detection in News Corpora</a></p>
<p>
  Main idea of this paper is to introduce a sentence-based containment detection method which is suitable for streaming news articles. There are several methods that use different similarity measures to find the near duplicate documents. Some of these methods include using all of the words in the documents, some include using a sequence of each word. 
  Shingling can be given as an example to the latter approach. If two documents have common shingles above a threshold, they can be considered as near duplicates (similar). 
  According to this paper, shingling used approaches suffer from efficiency issues, it is because of comparing each shingle of a document with shingles of other documents in the dataset.
  A unique similarity measure called Containment Similarity (CS) is used in the algorithm described in this paper. CS measures to what extent document_a is contained by document_d by using the set of sentences in those documents and inverse document frequency of the longest word prefix match of the sentences in the sets. 
  As the word prefix match of the sentences gets longer, similarity between these sentences increases.
  Corpus tree structure is used for processing and calculating the CS of the sentences of documents in the dataset. CoDet determines the containment of two documents by using the ratio between CS of two documents efficiently and effectively.
</p>
<p><a href="http://www2007.cpsc.ucalgary.ca/papers/paper215.pdf">Detecting Near-duplicates for Web Crawling</a></p>
<p>
  According to this paper, two web documents can be identical in terms of their contents but they can differ in a small portion, such as advertisements, counters and timestamps. These differences are irrelevant for web search. Elimination of near duplicate documents saves network bandwith, reduces storage costs and improves the quality of search indexes.
  The algorithm presented in this paper uses Charikar’s SimHash algorithm, which is useful for identifying near duplicates in web documents belonging to a multi-billion page repository. SimHash is a fingerprinting technique that depends on the following property: near duplicate documents differ in a small number of bit positions. 
  The authors also develop a technique for finding all fingerprints that differ from a given fingerprint in at most k bit positions in a collection of f-bit fingerprints (Hamming Distance Problem). 
  Their first algorithm works as follows: documents are converted into a set of features, which are tagged with its weight; using tokenization, stop-word removal, stemming, and phrase detection techniques. Each feature is hashed into an f-bit hash value. These f bits increment or decrement the weight of that feature according to the bit value of i-th bit. For instance, if the i-th bit of the hash value is 0, the i-th component of f-dimensional vector is decremented by the weight of that feature. The signs of the components after all features have been processed, determine the corresponding bits of the final fingerprint.
  The authors try to address the problem for determining similar documents with similar hash values, contrary to the cryptographic hash functions such as SHA-1 or MD5. These hash functions are chosen for the property that slightly different inputs have potentially very different outputs, which makes SimHash unique.
  It is stated that, their algorithm works best when their fingerprints differ in at most 3 bits. The distributions of fingerprints are also analysed by the authors. 
</p>

<p><a href="http://ir.cs.georgetown.edu/publications/downloads/p171-chowdhury.pdf">Collection Statistics for Fast Duplicate Document Detection</a></p>
<p>
  If the similarity measure of a document to the other is high, they are considered as similar documents. The approach of using the document as a query is infeasible for large collections because each document must be queried against the entire collection. Idf method was used for several algorithms in the past to weight which terms should be used for original query/document. According to the authors, this approach is also not feasible for large documents because of I/O costs to retrieve all documents for the posting list to be analysed. Therefore, they developed the I-Match algorithm to eliminate I/O costs and finding duplicate documents in a more efficient way.
  I-Match uses collection statistics to identify which terms should be used as a basis for comparison of the documents using the inversed document frequency of each term in order to determine the usefulness of these terms.  Their approach hinges on the premise that removal of very infrequent terms or very common terms results in good document representations for identifying duplicates.
  I-Match yields a higher percentage of detection than Digital Syntactic Clustering (DSC) or DSC super shingle approaches. It also yields a smaller number of document clusters than any of the shingling techniques. Besides, no false positives are detected with the I-Match algorithm. 
</p>

<p><a href="http://acberg.com/bigdata/papers/broder_shingling.pdf">Syntactic Clustering of the Web</a></p>

<h2>
<a id="meetings" class="anchor" href="#meetings" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Meetings</h2>

<p>1) January 25, 2016 in EA511: Project proposal report is revised and the topic of the project is narrowed down.</p>
<p>2) February 2, 2016 in EA511: Project proposal report is revised and additional papers are suggested by Fazlı Can.</p>
<p>3) February 5, 2016 in EA511: "GroundHog Day: Near-Duplicate Detection on Twitter" and "Efficient Semantic-Aware Detection of Near Duplicate Document Detection" papers are discussed with Fazlı Can.</p>

<h2>
<a id="quick-links" class="anchor" href="#quick-links" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Quick Links</h2>

<p><a href="http://cs.bilkent.edu.tr/">Bilkent University Computer Engineering Department</a></p>

<p><a href="http://www.cs.bilkent.edu.tr/%7Ecanf/">Prof. Fazlı Can's Homepage</a></p>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Bilkent University CS490 Research Project maintained by <a href="https://github.com/alinaciuysal">alinaciuysal</a></p>
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
