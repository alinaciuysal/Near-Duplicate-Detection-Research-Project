<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Bilkent University CS490 Research Project : ">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Bilkent University CS490 Research Project</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/alinaciuysal/ndd_project">View on GitHub</a>

          <h1 id="project_title">Bilkent University CS490 Research Project</h1>
          <h2 id="project_tagline"></h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/alinaciuysal/ndd_project/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/alinaciuysal/ndd_project/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h1>
<a id="semantic-duplicate-detection-of-tweets" class="anchor" href="#semantic-duplicate-detection-of-tweets" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Semantic-duplicate Detection of Tweets</h1>

<p>Duplicate document detection topic has been studied for years in the Information Retrieval field of Computer Science. There exist "state-of-art" algorithms for detecting near-duplicate documents efficiently.</p>

<p>In this project, the near-duplicate English and Turkish tweets posted on Twitter in a specific time-line will be found using our unique algorithm.</p>

<p>Student Name: Ali Naci Uysal</p>

<p>Project Supervisor: Prof. Fazlı Can</p>

<h2>
<a id="related-papers" class="anchor" href="#papers" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Related Papers</h2>

<p><a href="http://livingknowledge.europarchive.org/images/publications/semAwareNDD.pdf">Efficient Semantic-Aware Detection of Near Duplicate Resources</a></p>
<p>The Semantic Web provides a common framework that allows data to be shared and reused across application, enterprise, and community boundaries. 
Meta-data of resources (news articles, blog postings etc.) are stored in RDF (Resource Description Framework) format. Calais Web Service can extract following information from resources:
Entities, relationships, facts, events, and topics. Calais Web Service then processes the information extracted from the text and returns semantic metadata in RDF format.
The RDF structure of news articles are stored in a graph structure in this article. Locality Sensitive Hashing (LSH) and Jaccard Coefficient is applied to find the similarities of these graphs in order not to do pairwise comparisons. 
The indexing structure also makes their algorithm more efficient.
</p>

<p><a href="http://www2013.wwwconference.org/proceedings/p1273.pdf">Groundhog Day: Near-Duplicate Detection on Twitter</a></p>
<p> In this article, Tweet pairs are examined by using following features:
<p> Syntactic features: Manhattan Distance, overlap in terms of tweets, in hash tags, URLs, tweet length difference </p>
<p> Semantic features: Overlap in entities, entity types, topics (using DBpedia Spotlight and OpenCalais), WordNet concepts and WordNet similarity </p>  
<p> Enriched Semantic Features are constructed as in the same way as semantic features, but it also includes contents of web sites that are linked from the tweets. </p>
<p> Contextual Features: Temporal difference between tweet pairs, user similarity (the differences in #followers and #followees) </p>
WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. The authors identified nouns in the pairs of tweets and calculated their overlap in these nouns by using Java Wordnet Interface to find the root concepts of the nouns in the tweet pairs.
It is stated that, “overlap of WordNet concepts” is a good criterion to detect duplicate tweets compared to other features. </p>
</p>

<p><a href="http://www.cs.bilkent.edu.tr/%7Ecanf/publications/CoDet%20CIKM%202011.pdf">CoDet: Sentence-based Containment Detection in News Corpora</a></p>
<p>
  Main idea of this paper is to introduce a sentence-based containment detection method which is suitable for streaming news articles. There are several methods that use different similarity measures to find the near duplicate documents. Some of these methods include using all of the words in the documents, some include using a sequence of each word. 
  Shingling can be given as an example to the latter approach. If two documents have common shingles above a threshold, they can be considered as near duplicates (similar). 
  According to this paper, shingling used approaches suffer from efficiency issues, it is because of comparing each shingle of a document with shingles of other documents in the dataset.
  A unique similarity measure called Containment Similarity (CS) is used in the algorithm described in this paper. CS measures to what extent document_a is contained by document_d by using the set of sentences in those documents and inverse document frequency of the longest word prefix match of the sentences in the sets. 
  As the word prefix match of the sentences gets longer, similarity between these sentences increases.
  Corpus tree structure is used for processing and calculating the CS of the sentences of documents in the dataset. CoDet determines the containment of two documents by using the ratio between CS of two documents efficiently and effectively.
</p>
<p><a href="http://www2007.cpsc.ucalgary.ca/papers/paper215.pdf">Detecting Near-duplicates for Web Crawling</a></p>
<p>
  According to this paper, two web documents can be identical in terms of their contents but they can differ in a small portion, such as advertisements, counters and timestamps. These differences are irrelevant for web search. Elimination of near duplicate documents saves network bandwith, reduces storage costs and improves the quality of search indexes.
  The algorithm presented in this paper uses Charikar’s SimHash algorithm, which is useful for identifying near duplicates in web documents belonging to a multi-billion page repository. SimHash is a fingerprinting technique that depends on the following property: near duplicate documents differ in a small number of bit positions. 
  The authors also develop a technique for finding all fingerprints that differ from a given fingerprint in at most k bit positions in a collection of f-bit fingerprints (Hamming Distance Problem). 
  Their first algorithm works as follows: documents are converted into a set of features, which are tagged with its weight; using tokenization, stop-word removal, stemming, and phrase detection techniques. Each feature is hashed into an f-bit hash value. These f bits increment or decrement the weight of that feature according to the bit value of i-th bit. For instance, if the i-th bit of the hash value is 0, the i-th component of f-dimensional vector is decremented by the weight of that feature. The signs of the components after all features have been processed, determine the corresponding bits of the final fingerprint.
  The authors try to address the problem for determining similar documents with similar hash values, contrary to the cryptographic hash functions such as SHA-1 or MD5. These hash functions are chosen for the property that slightly different inputs have potentially very different outputs, which makes SimHash unique.
  It is stated that, their algorithm works best when their fingerprints differ in at most 3 bits. The distributions of fingerprints are also analysed by the authors. 
</p>

<p><a href="http://ir.cs.georgetown.edu/publications/downloads/p171-chowdhury.pdf">Collection Statistics for Fast Duplicate Document Detection</a></p>
<p>
  If the similarity measure of a document to the other is high, they are considered as similar documents. The approach of using the document as a query is infeasible for large collections because each document must be queried against the entire collection. Idf method was used for several algorithms in the past to weight which terms should be used for original query/document. According to the authors, this approach is also not feasible for large documents because of I/O costs to retrieve all documents for the posting list to be analysed. Therefore, they developed the I-Match algorithm to eliminate I/O costs and finding duplicate documents in a more efficient way.
  I-Match uses collection statistics to identify which terms should be used as a basis for comparison of the documents using the inversed document frequency of each term in order to determine the usefulness of these terms.  Their approach hinges on the premise that removal of very infrequent terms or very common terms results in good document representations for identifying duplicates.
  I-Match yields a higher percentage of detection than Digital Syntactic Clustering (DSC) or DSC super shingle approaches. It also yields a smaller number of document clusters than any of the shingling techniques. Besides, no false positives are detected with the I-Match algorithm. 
</p>
<p>
  To group the tweets in our own dataset, I will be using Polyglot Python library, which can be found at: <a href="http://polyglot.readthedocs.org/en/latest/">polyglot.readthedocs.org/en/latest/</a>
</p>

<p><a href="http://www.cs.bilkent.edu.tr/~canf/bilir_web/theses/erkanUyarThesis.pdf">Near-Duplicate News Detection Using Named Entities</a></p>
<p>
  Uyar et al. developed an algorithm called Tweezer to create document signatures using Inverse Document Frequency values of terms that are in news articles. It is stated that, Tweezer is better than I-Match algorithm in terms of effectiveness and efficiency. 
  It is more effective than I-Match by using a cost function that combines false alarm and miss rate probabilities, it is also 7% faster than I-Match in terms of efficiency. The signature creation algorithm is based on the following structure: prefix-NamedEntity-suffix. It is stated that, using only named entities will not perform good enough in detecting near-duplicates because of false positive rates. 
  They also use Shingling approach to create shingles of signatures, and SHA1 hash function to hash shingles of documents and relate them with those documents. 
  To detect near-duplicates, they compare hashes of a news article with hashes of other news articles. Similarity between two news aticles increases as the number of same hashes between two news articles increase.
</p>

<p><a href="http://alt.qcri.org/semeval2015/cdrom/index.html">Sem-Eval-2015 Task 1: Paraphrase and Semantic Similarity in Twitter (PIT)</a></p>
<p>In this task above, 19 teams developed different algorithms for detecting semantic similarity of tweets in a given dataset. First two of the best algorithms (according to Pearson correlation scores) are: ASOBEK and RTM-DCU systems. </p>


<h2>
<a id="meetings" class="anchor" href="#meetings" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Meetings</h2>

<p>1) January 25, 2016 in EA511: Project proposal report is revised and the topic of the project is narrowed down.</p>
<p>2) February 2, 2016 in EA511: Project proposal report is revised and additional papers are suggested by Fazlı Can.</p>
<p>3) February 5, 2016 in EA511: "GroundHog Day: Near-Duplicate Detection on Twitter" and "Efficient Semantic-Aware Detection of Near Duplicate Document Detection" papers are discussed with Fazlı Can.</p>
<p>4) February 12, 2016 in EA507: Cagri Toraman gave a tweet sample and some advices to develop an algorithm for finding semantic similarity between tweet pairs.</p>
<p>5) February 19, 2016 in EA511: Following objectives are determined for next week's meeting: 1) Baseline algorithms, their papers and used datasets will be examined. 2) Tweets in the sample dataset will be grouped according to their languages.</p>
<p>6) February 26, 2016 in EA511: Dataset of "Groundhog Day: Near-Duplicate Detection on Twitter" paper is requested from NIST.</p>
<p>7) March 1, 2016 in EA511: New algorithms for proper dataset creation are discussed with Fazlı Can and Çağrı Toraman. </p>
<p>8) March 8, 2016 in EA511: Details of Tubitak dataset, which includes tweets that are seperated according to their languages with the help of Polyglot Library, are discussed.</p>
<p>9) March 18, 2016 in EA507: Different approaches to use semantic features of tweets are discussed with Çağrı Toraman and Fazlı Can.</p>
<p>10) March 30, 2016 in EA507: Pre-processing stages, stemming stages, and 3 different algorithms to detect near-duplicate Turkish tweets are discussed with Çağrı Toraman. First one uses Google search engine queries to detect the relevancy between two words of tweets.
    Second one is an algorithm that uses a social network that is based on a graph structure, and the last one is about finding WordNet concepts in tweets after translating words in Turkish tweets to English, where we will use 4 different semantic features to detect near-duplicate English tweets in our dataset.
</p>
<p>11) April 6, 2016 in the campus with Fazlı Can: Current situation of this research project, results of SimHash algorithm, dataset generation algorithm, pre-processing stages in Turkish tweets,  problems in parsing Turkish Wikipedia articles to generate above-mentioned graph structure are discussed with Fazlı Can. 
He also suggested using windowing approaches in order to reduce the time required to detect near-duplicate Turkish tweets in our dataset because pairing every tweet will result in approximately 8 billion tweet pairs. </p>
<p>12) April 30, 2016 in EA507: Pre-processing stages, stemming stages, and an algorithm to detect near-duplicate English and Turkish tweets are discussed with Çağrı Toraman. This algorithm is based on word association network structure.
</p>
<p>13) May 4, 2016 in EA507: Enhancements to our algorithm and baseline algorithm is discussed with Çağrı Toraman.
</p>
<p>14) May 11, 2016 in EA507: Conducted experiments and their results are discussed with Fazlı Can and Çağrı Toraman.
</p>
<h2>
<a id="quick-links" class="anchor" href="#quick-links" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Quick Links</h2>

<p><a href="http://cs.bilkent.edu.tr/">Bilkent University Computer Engineering Department</a></p>

<p><a href="http://www.cs.bilkent.edu.tr/%7Ecanf/">Prof. Fazlı Can's Homepage</a></p>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Bilkent University CS490 Research Project maintained by <a href="https://github.com/alinaciuysal">alinaciuysal</a></p>
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
